# -*- coding: utf-8 -*-
"""TF2.0 CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14trbjIGWe83i0_hZzQTfw4xc1KSHZNpB
"""

## Import necessary libs
import tensorflow as tf

"""### Get the data"""

import zipfile

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip

# Unzip the downloaded file
zip_ref = zipfile.ZipFile("pizza_steak.zip")
zip_ref.extractall()
zip_ref.close()

"""#### Inspect the data"""

!ls pizza_steak

!ls pizza_steak/train

!ls pizza_steak/test

import os

# walkthrough the data
for dirpath, dirnames, filenames in os.walk("pizza_steak"):
  print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}." )

num_steak_images_train = len( os.listdir("pizza_steak/train/steak"))
num_steak_images_train

## To visualize the images, lets get the class names programmatically
import pathlib
import numpy as np

data_dir = pathlib.Path("pizza_steak/train")
class_names = np.array(sorted([item.name for item in data_dir.glob("*")]))
print(class_names)

# Lest visualize our image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random

def view_random_image(target_dir, target_class):
  # Set up the target directory
  target_folder = target_dir + target_class

  # get the random image
  random_image = random.sample(os.listdir(target_folder), 1)

  # Read image and plot
  img  = mpimg.imread(target_folder + "/" + random_image[0])
  plt.imshow(img)
  plt.title(target_class)
  plt.axis("off")

  print(f"Image Shape : {img.shape}")
  return img

# View random image from training dataset
img = view_random_image(target_dir = "pizza_steak/train/", target_class="steak")

# View random image from training dataset
img = view_random_image(target_dir = "pizza_steak/train/", target_class="pizza")

# View the image shape
img.shape # returns (width, height, colour channels)

# Get all the pixel values between 0 & 1
img/255.

"""### End To End Convolution Neural Network

- Load images
- Preprocess images
- Build CNN
- Compile CNN
- Fit the training data

#### Model 1
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# set the random seed
tf.random.set_seed(42)

# Preprocess the image data (Scaling / Normalization)
train_datagen = ImageDataGenerator(rescale=1./255)
valid_datagen = ImageDataGenerator(rescale=1./255)

# Set path to the image data
train_dir = "/content/pizza_steak/train"
test_dir = "/content/pizza_steak/test"

# Import data from directories and turn it to batches
train_data = train_datagen.flow_from_directory(train_dir,
                                               batch_size=32, # number of images to process at a time
                                               target_size=(224, 224), # convert all images to be 224 x 224
                                               class_mode="binary", # type of problem we're working on
                                               seed=42)

valid_data = valid_datagen.flow_from_directory(test_dir,
                                               batch_size=32,
                                               target_size=(224, 224),
                                               class_mode="binary",
                                               seed=42)

## Build the CNN architecture
model_1 = tf.keras.Sequential([
    tf.keras.layers.Conv2D(filters=10,
                           kernel_size=3,
                           activation="relu",
                           input_shape=(224, 224, 3)),
    tf.keras.layers.Conv2D(10, 3, activation="relu"),
    tf.keras.layers.MaxPool2D(pool_size=(2,2),
                                 padding="valid"),
    tf.keras.layers.Conv2D(10, 3, activation="relu"),
    tf.keras.layers.Conv2D(10, 3, activation="relu"),
    tf.keras.layers.MaxPool2D(2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1, activation="sigmoid")

])

model_1.compile(loss= "binary_crossentropy",
                optimizer= tf.keras.optimizers.Adam(),
                metrics= ["accuracy"])

history_1 = model_1.fit(train_data,
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=valid_data,
                        validation_steps=len(valid_data))

# GEt the model summary
model_1.summary()

"""#### Model 2 - Using Dense Layers"""

# Set random seed
tf.random.set_seed(42)

# Create a model to replicate the TensorFlow Playground model
model_2 = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model_2.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])

# Fit the model
history_2 = model_2.fit(train_data, # use same training data created above
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=valid_data, # use same validation data created above
                        validation_steps=len(valid_data))

# Check out our second model's architecture
model_2.summary()

"""#### Model 3 - Stepping Up"""

# set the random seeed
tf.random.set_seed(42)

# Create the model
model_3 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape = (224, 224, 3)),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

# Compile the model
model_3.compile(loss = "binary_crossentropy",
                optimizer = tf.keras.optimizers.Adam(),
                metrics = ['accuracy'])

# fit the model
history_3 = model_3.fit(train_data,
                        epochs = 5,
                        validation_data = valid_data)

# Model summary
model_3.summary()

"""### Binary classification: Let's break it down
We just went through a whirlwind of steps:

1. Become one with the data (visualize, visualize, visualize...)
2. Preprocess the data (prepare it for a model)
3. Create a model (start with a baseline)
4. Fit the model
5. Evaluate the model
6. Adjust different parameters and improve model (try to beat your baseline)
Repeat until satisfied

Let's step through each.

#### 1. Become one with data
"""

# Visualize the data
plt.figure()
plt.subplot(1, 2, 1)
steak_img = view_random_image("pizza_steak/train/","steak")
plt.subplot(1, 2, 2,)
pizza_img = view_random_image("pizza_steak/train/", "pizza")

"""#### 2. Preprocess the data"""

# define our directory dataset paths
train_dir = "pizza_steak/train"
test_dir = "pizza_steak/test"

# Create train and test data generators and rescale the data
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1 / 255. )
valid_datagem = ImageDataGenerator(rescale = 1 / 255. )

# Load the data from directories and change them into batches
train_data = train_datagen.flow_from_directory(directory = train_dir,
                                               target_size = (224, 224),
                                               class_mode = "binary",
                                               batch_size = 32)

valid_data = valid_datagem.flow_from_directory(directory = test_dir,
                                               target_size = (224, 224),
                                               class_mode = "binary",
                                               batch_size = 32)

# Get a sample of train data batch
images, labels = train_data.next()
len(images) , len(labels)

# How many batches are there
len(train_data)

# Get the first two images
images[:2], images[0].shape

# View the batch of labels
labels

"""#### 3. Create the model

##### Baseline Model
"""

# Make creating model easier
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation
from tensorflow.keras import Sequential

# Create the model
model_4 = Sequential([
    Conv2D(filters=10, kernel_size=3, activation="relu", strides=1, padding="valid", input_shape=(224,224,3)),
    Conv2D(10, 3, activation="relu"),
    Conv2D(10, 3, activation="relu"),
    Flatten(),
    Dense(1, activation="sigmoid")
])

model_4.compile(loss = "binary_crossentropy",
                optimizer = Adam(),
                metrics=["accuracy"])

"""#### 4. Fit the model"""

history_4 = model_4.fit(train_data,
            validation_data = valid_data,
            epochs = 5)

"""#### Evaluate the model"""

model_4.evaluate(valid_data)

import pandas as pd
pd.DataFrame(history_4.history).plot(figsize = (10,7))

# Plot the validation and training data separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

# Check out the loss curves of model_4
plot_loss_curves(history_4)

# Check out our model's architecture
model_4.summary()

"""#### 6. Adjust the model parameters

To induce the overfitting:
- Increase the number of CONV layers
- Increase the number of CONV filters
- Add another dense layer to the flattened output

To reduce overfitting:
- Add Data Augmentation
- Add Regularization layer (such as MaxPool2D)
- Add more data
"""

# create the model
model_5 = Sequential([
    Conv2D(filters = 10, kernel_size=3, activation ="relu", input_shape = (224,224,3)),
    MaxPool2D(pool_size =(2,2)),
    Conv2D(10, 3, activation= "relu"),
    MaxPool2D(),
    Conv2D(10, 3, activation="relu"),
    Flatten(),
    Dense(1, activation="sigmoid")
])

model_5.compile(loss = "binary_crossentropy",
                optimizer = Adam(),
                metrics = ['accuracy'])

history_5 = model_5.fit(train_data,
                        validation_data = valid_data,
                        epochs =5)

model_5.summary()

# plot loss curve
plot_loss_curves(history_5)

"""#### Data Augmentation"""

# Create ImageDataGenerator training instance with data augmentation
train_datagen_augmented = ImageDataGenerator(rescale=1/255.,
                                             rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)
                                             shear_range=0.2, # shear the image
                                             zoom_range=0.2, # zoom into the image
                                             width_shift_range=0.2, # shift the image width ways
                                             height_shift_range=0.2, # shift the image height ways
                                             horizontal_flip=True) # flip the image on the horizontal axis

# Create ImageDataGenerator training instance without data augmentation
train_datagen = ImageDataGenerator(rescale=1/255.)

# Create ImageDataGenerator test instance without data augmentation
test_datagen = ImageDataGenerator(rescale=1/255.)

print("Data Augmeneted Dataset")
train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,
                                                                   target_size = (224, 224),
                                                                   class_mode = "binary",
                                                                   batch_size = 32,
                                                                   shuffle = False)

print("Non Augmented Train Data")
train_data = train_datagen.flow_from_directory(train_dir,
                                               target_size =(224, 224),
                                               class_mode = "binary",
                                               batch_size= 32,
                                               shuffle = False)

print("Non Augmented Test Data")
valid_data = test_datagen.flow_from_directory(test_dir,
                                              target_size = (224, 224),
                                              class_mode = "binary",

                                              batch_size = 32)

# Get data batch samples
images, labels = train_data.next()
augmented_images, augmented_labels = train_data_augmented.next() # Note: labels aren't augmented, they stay the same

# Show original image and augmented image
random_number = random.randint(0, 31) # we're making batches of size 32, so we'll get a random instance
plt.imshow(images[random_number])
plt.title(f"Original image")
plt.axis(False)
plt.figure()
plt.imshow(augmented_images[random_number])
plt.title(f"Augmented image")
plt.axis(False);

