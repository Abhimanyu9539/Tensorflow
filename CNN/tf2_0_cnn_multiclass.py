# -*- coding: utf-8 -*-
"""TF2.0 CNN Multiclass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MtDYjeaXvlWWWnW7PQVC9ORqcNT-XDE5

### Import the data
"""

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip

# Unzip the data
import zipfile
zip_ref = zipfile.ZipFile("10_food_classes_all_data.zip","r")
zip_ref.extractall()
zip_ref.close()

import os

# wwalkthrough the data
for dirpath, dirnames, filenames in os.walk("10_food_classes_all_data"):
  print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'")

# set up the train and test directories
train_dir = "10_food_classes_all_data/train/"
test_dir = "10_food_classes_all_data/test/"

# Lets get the subdirectories (These are classnames)
import pathlib
import numpy as np

data_dir = pathlib.Path(train_dir)
class_names = np.array(sorted([item.name for item in data_dir.glob("*")]))
print(class_names)

# Visualize the data
# View an image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random

def view_random_image(target_dir, target_class):
  '''
  Input:
  target_dir : target directory
  target_class : target classes

  Output:
  Prints the random image from the providied directory
  '''
  # Setup target directory (we'll view images from here)
  target_folder = target_dir+target_class

  # Get a random image path
  random_image = random.sample(os.listdir(target_folder), 1)
  print(random_image)

  # Read in the image and plot it using matplotlib
  img = mpimg.imread(target_folder + "/" + random_image[0])
  plt.imshow(img)
  plt.title(target_class)
  plt.axis("off");

  print(f"Image shape: {img.shape}") # show the shape of the image

  return img

img = view_random_image(target_dir = train_dir, target_class = random.choice(class_names))

# Plot the validation and training data separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();



"""### Preprocess the data"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Rescale the data
train_datagen = ImageDataGenerator(rescale = 1/255.0 )
test_datagen = ImageDataGenerator(rescale = 1/255.0 )

# Load the data from directories and turn them into batches
train_data = train_datagen.flow_from_directory(train_dir,
                                               target_size = (224, 224),
                                               class_mode = "categorical",
                                               batch_size = 32)

test_data = test_datagen.flow_from_directory(test_dir,
                                             target_size = (224, 224),
                                             class_mode = "categorical",
                                             batch_size = 32)

"""### Create the model

#### Baseline Model
"""

from tensorflow.keras import Sequential
from tensorflow.keras.layers import  Dense, Flatten, Conv2D, MaxPool2D, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import categorical_crossentropy

# Create the model
model_1 = Sequential([
    Conv2D(10, 3, activation = "relu", input_shape =(224, 224, 3)),
    MaxPool2D(pool_size = (2,2)),
    Conv2D(10, 3, activation = "relu"),
    MaxPool2D(pool_size = (2,2)),
    Conv2D(10, 3, activation= "relu"),
    MaxPool2D(),
    Flatten(),
    Dense(10, activation = "softmax")
])

# Compile the model
model_1.compile(loss = "categorical_crossentropy",
                optimizer = Adam(),
                metrics = ['accuracy'])

# Fit the model
history_1 = model_1.fit(train_data,
                        validation_data = test_data,
                        epochs = 5)

# summary
model_1.summary()

# evaluate the model
model_1.evaluate(test_data)

# plot the graphs
plot_loss_curves(history_1)

"""### Adjust the model hyperparameters"""

## Simplify the model by  removing 2 conv layers
model_2 = Sequential([
    Conv2D(10, 3, activation="relu", input_shape=(224,224,3)),
    MaxPool2D(),
    Conv2D(10, 3, activation = "relu"),
    MaxPool2D(),
    Flatten(),
    Dense(10, activation="softmax")
])

# Compile
model_2.compile(loss = "categorical_crossentropy",
                optimizer = Adam(),
                metrics = ['accuracy'])

# Fit
history_2 = model_2.fit(train_data,
                        validation_data = test_data,
                        epochs =5)

plot_loss_curves(history_2)

"""Not much change in the output
Model 2 parameter count has increased from model 1 parameters count.

### Use Augmentation for the data
"""

print("Generating augmented data")
train_datagen_augmented = ImageDataGenerator(rescale=1/255.,
                                             rotation_range=20, # note: this is an int not a float
                                             width_shift_range=0.2,
                                             height_shift_range=0.2,
                                             zoom_range=0.2,
                                             horizontal_flip=True)

train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,
                                                                   target_size = (224, 224),
                                                                   class_mode= "categorical",
                                                                   batch_size = 32)

# Lets create the model but this time we will overfit it on the augmented data
import tensorflow as tf

model_3 = tf.keras.models.clone_model(model_1)

# Compile the cloned model
model_3.compile(loss = "categorical_crossentropy",
                optimizer = Adam(),
                metrics = ['accuracy'])

model_3.summary()

# Fit the model
history_3 = model_3.fit(train_data_augmented,
                        validation_data = test_data,
                        epochs = 5)

plot_loss_curves(history_3)

"""### Making the predictions"""

class_names

!wget -q https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/images/03-hamburger.jpeg?raw=true
!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg
!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg
!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg
!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg

# Create a function to import an image and resize it to be able to be used with our model
def load_and_prep_image(filename, img_shape=224):
  """
  Reads an image from filename, turns it into a tensor
  and reshapes it to (img_shape, img_shape, colour_channel).
  """
  # Read in target file (an image)
  img = tf.io.read_file(filename)

  # Decode the read file into a tensor & ensure 3 colour channels
  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)
  img = tf.image.decode_image(img, channels=3)

  # Resize the image (to the same size our model was trained on)
  img = tf.image.resize(img, size = [img_shape, img_shape])

  # Rescale the image (get all values between 0 and 1)
  img = img/255.
  return img

def pred_and_plot(model, filename, class_names):
  """
  Imports an image located at filename, makes a prediction on it with
  a trained model and plots the image with the predicted class as the title.
  """
  # Import the target image and preprocess it
  img = load_and_prep_image(filename)

  # Make a prediction
  pred = model.predict(tf.expand_dims(img, axis=0))

  if len(pred[0]) > 1:
    pred_class = class_names[tf.argmax(pred[0])]
  else:
    pred_class = class_names[int(tf.round(pred)[0][0])]

  # Plot the image and predicted class
  plt.imshow(img)
  plt.title(f"Prediction: {pred_class}")
  plt.axis(False);

# Make a prediction using model_11
pred_and_plot(model=model_3,
              filename="03-steak.jpeg",
              class_names=class_names)

# Make a prediction using model_11
pred_and_plot(model=model_3,
              filename="03-sushi.jpeg",
              class_names=class_names)

# Make a prediction using model_11
pred_and_plot(model=model_3,
              filename="03-hamburger.jpeg",
              class_names=class_names)

# Make a prediction using model_11
pred_and_plot(model=model_3,
              filename="03-pizza-dad.jpeg",
              class_names=class_names)

"""### Saving and Loading the model"""

## Save the model
model_3.save("Saved_Model_3")

# Load the model
loaded_model = tf.keras.models.load_model("Saved_Model_3")
loaded_model.evaluate(test_data)

model_3.evaluate(test_data)